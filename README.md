# DocChat â€” AI Document Assistant

A RAG (Retrieval-Augmented Generation) application that allows you to upload PDF documents while using local LLMs for information retrieval. Built with FastAPI, React, LangChain, and FAISS for efficient document embedding and vector similarity search.

## Features

- ðŸ“„ Upload PDF documents
- ðŸ§  Automatic embedding generation with Ollama
- ðŸ’¬ Chat interface to ask questions about your documents
- ðŸ” Source citations for answers
- âš¡ Fast vector search with FAISS

## Project Structure

```
rag-env/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app.py          # FastAPI server
â”‚   â”œâ”€â”€ ingest.py       # PDF processing & embedding
â”‚   â”œâ”€â”€ query.py        # Vector search
â”‚   â”œâ”€â”€ chat.py         # LLM chat integration
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.jsx     # Main React component
â”‚   â”‚   â”œâ”€â”€ main.jsx    # Entry point
â”‚   â”‚   â””â”€â”€ index.css   # Tailwind styles
â”‚   â””â”€â”€ package.json
â”œâ”€â”€ pdf_inputs/         # Uploaded PDFs
â””â”€â”€ vectorstore/        # FAISS index
```

## Prerequisites

- Python 3.11+
- Node.js 18+
- [Ollama](https://ollama.ai) installed locally

> **Note for Docker users**: Allocate at least 6GB of memory to Docker Desktop (Settings â†’ Resources â†’ Advanced) for smooth operation.

### Install Ollama Models

```bash
# Embedding model (required)
ollama pull nomic-embed-text

# Chat model
ollama pull gemma3:4b
```

## Setup

### Backend

```bash
cd backend

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Start the server
python app.py
```

The API will be available at `http://localhost:8000`

### Frontend

```bash
cd frontend

# Install dependencies
npm install

# Start development server
npm run dev
```

The app will be available at `http://localhost:5173`

## Usage

1. **Upload PDFs**: Drag or click to upload your PDF documents
2. **Process Documents**: Click "Process Documents" to generate embeddings
3. **Start Chatting**: Ask questions about your documents

## API Endpoints

| Method | Endpoint | Description |
|--------|----------|-------------|
| GET | `/pdfs` | List uploaded PDFs |
| POST | `/upload` | Upload a PDF file |
| DELETE | `/pdfs/{filename}` | Delete a PDF |
| POST | `/ingest` | Process all PDFs |
| POST | `/chat` | Chat with documents |

## Tech Stack

**Backend:**
- FastAPI
- LangChain
- FAISS
- Ollama

**Frontend:**
- React + Vite
- Tailwind CSS
- Lucide Icons

## Docker Setup

Run the entire application using Docker - includes Ollama and all dependencies.

### Quick Start:

1. **Navigate to project directory:**
   ```bash
   cd /path/to/rag-env
   ```

2. **Build and start all services:**
   ```bash
   docker-compose up --build
   ```

3. **Access the application:**
   - Frontend: http://localhost:3000
   - Backend API: http://localhost:8000
   - API Docs: http://localhost:8000/docs

### First Run:

On first startup, the system will automatically download and set up:
- Ollama server
- Required models (`nomic-embed-text` and `gemma3:4b`)

**First run takes 5-10 minutes** to download models (~3-4GB). Subsequent runs are fast.

### Docker Commands:

```bash
# Start in background
docker-compose up -d

# View logs
docker-compose logs -f

# Stop containers
docker-compose down

# Rebuild after code changes
docker-compose up --build

# Remove everything (including models)
docker-compose down -v
```

### Memory Configuration:

1. Open Docker Desktop
2. Go to **Settings** â†’ **Resources** â†’ **Advanced**
3. Set **Memory** to at least **6GB** (8GB+ recommended)
4. Click **Apply & Restart**

See [DOCKER.md](./DOCKER.md) for detailed Docker documentation.
